---
title: "Session 2: setting up graphical applications"
teaching: 10
exercises: 50
questions:
objectives:
---


### Goals

In this session, we're having a look at examples of how to setup and run containerised applications that feature a graphical user interface (GUI).  

In particular we'll cover:
* an RStudio web server (with a phylogenetic tree package)
* a Jupyter Notebook web server (with a visualisation library for interaction networks)
* an X11 windowed application (Gnuplot, a data analysis utility)

For the first two exercises, we're going to review the following Singularity syntax:
* `singularity exec` to run commands from containers;
* some *runtime flags* for `exec`, namely `-B` and `-C`;
* *shell variables* for Singularity, *i.e.* prepended with `SINGULARITYENV_`.

With the third exercise, I will discuss what are the requirements to run an X11 application in containers, and guide you through a small case.

The RStudio example follows the webinar episode [GUI enabled web applications: RStudio in a container](https://pawseysc.github.io/singularity-containers/23-web-rstudio/index.html).  Here, however, you'll go in greater detail, and proceed step by step yourself.


### GUIDED - Run a containerised R analysis from command line

Let's cd in the appropriate directory:

```
$ cd /data/containers-bioinformatics-workshop
$ export WORK=$(pwd)
$ cd exercises/graphical
```
{: .bash}

Now, suppose we need to set an R analysis pipeline.  We will need to run it on different systems, both via command line interface (CLI) and RStudio, and even share the protocol with other collaborators.  Hence using a container seems a great choice to improve portability and reproducibility.  

The R script is at `home_rstudio/script.r`.  It creates a graphical representation of a very simple phylogenetic tree.  This is based on the example on [Chapter 5](https://yulab-smu.github.io/treedata-book/chapter5.html) of the online book by Yu Lab.  
The required toolset consists of the `tidyverse` collection plus the `ggtree` package (this setup is used in the session on building as well).

We've already made the image available on Docker Hub: `marcodelapierre/ggtree:2.0.4`.  It's based on the image `rocker/tidyverse:3.6.1` by [Rocker](https://www.rocker-project.org).  Let's pull it:

```
$ singularity pull docker://marcodelapierre/ggtree:2.0.4
```
{: .bash}

You might now that every R installation ships with a utility to run scripts from the command line, so in this case we can go `Rscript home_rstudio/script.r`.  


> ## Let's run the R script through CLI
> 
> > ## Solution
> > 
> > ```
> > $ singularity exec ggtree_2.0.4.sif Rscript home_rstudio/script.r
> > ```
> > {: .bash}
> > 
> > We'll get an image file as an output, `tree.png`:
> > 
> > ```
> > $ ls -lh tree.png
> > ```
> > {: .bash}
> > 
> > ```
> > -rw-r--r-- 1 ubuntu ubuntu  96K Jun  9 05:52 tree.png
> > ```
> > {: .output}
> > 
> > To save time, I can show you the content of this file from my screen.
> {: .solution}
{: .challenge}


### ROOM - Spawn an interactive RStudio web session

All right, now it's your turn: I want you to work out the setup to run a containerised Rstudio session.  

Actually, you already saw the solution during the webinar, so getting it right is not the main point.  
The point is to stop at each step of its construction, to understand why it's needed.  You'll see a mix of arguments, relative both to the general containerised setup and to the specific way the `rocker` container you're using is setup.

At the end, you'll also be able to re-run the phylogenetic tree script you already executed through command line.

#### Hints
* you're using the same container as above: `ggtree_2.0.4.sif`;
* the command to start an Rstudio server is `rserver --www-port 8787 --www-address 0.0.0.0 --auth-none=0 --auth-pam-helper-path=pam-helper`;
* you'll embed it in a `singularity exec` command;
* you'll add flags to this command (see steps below);
* retrieve the IP address of your Nimbus VM;
* open the browser and go to the web address `http://<YOUR NIMBUS VM'S IP>:8787`; you'll monitor the outcome of your setup here.


> ## On the `rserver` command syntax
> 
> Why are we going to run the following?
> 
> ```
> rserver --www-port 8787 --www-address 0.0.0.0 --auth-none=0 --auth-pam-helper-path=pam-helper
> ```
> {: .source}
> 
> * `--www-port 8787` sets the communication port the Rstudio server will listen at for incoming connections; `8787` is a common choice for Rstudio;
> * `--www-address 0.0.0.0` is to let connections come from all IP addresses on the host machine, for instance not only local ones but also ones coming through the internet (this will be our case of usage);
> * `--auth-none=0` is to keep password authentication enabled, a wise security choice;
> * `--auth-pam-helper-path=pam-helper` is to use a specific utility for authentication, that allows to define the password as an environment variable prior to starting the server.
{: .callout}


> ## First attempt: plain
> 
> Just run the command above from within the `ggtree` container.  
> 
> > ## Solution
> > 
> > ```
> > $ singularity exec ggtree_2.0.4.sif \
> >     rserver --www-port 8787 --www-address 0.0.0.0 --auth-none=0 --auth-pam-helper-path=pam-helper
> > ```
> > {: .bash}
> > 
> > ```
> > 
> > ```
> > {: .output}
> {: .solution}
> 
> Now go to the browser and open the web address `http://<YOUR NIMBUS VM'S IP>:8787`.  What happens?
> 
> > ## Comments
> > 
> > You're getting a welcome page, good!  The server is up and running.
> > 
> > However, You're getting prompted for a username and password.  
> > You can get your username with `echo $USER`, or in alternative from the output of the command `whoami`.  In Nimbus Ubuntu VMs by default it's `ubuntu`.
> > 
> > However, you have no password to enter.
> > 
> > You can now kill the server from the terminal by hitting `Ctrl-C`.
> {: .solution}
{: .challenge}


> ## Second attempt: setup access credentials
> 
> This modification is required by the way RStudio works: if we want authentication on the web server, we need to define a `PASSWORD` variable in the shell environment.
> 
> Define the `PASSWORD` variable prior to re-running the `singularity exec` command.
> 
> **Hints**:
> * define it as `SINGULARITYENV_PASSWORD`, this is a good practice to ensure it's always passed in the container, even if we're enabling further container isolation;
> * define it by prepending `export`, as the variable needs to be available to `singularity`, which is a *sub-process* of the active shell;
> * for completeness, also define the USER name with `SINGULARITYENV_USER` in the same way, again to ensure this is defined also if the container has tighter isolation.
> 
> > ## Solution
> > 
> > ```
> > $ export SINGULARITYENV_PASSWORD="<Pick-Your-Password>"
> > $ export SINGULARITYENV_USER="$USER"
> > $ singularity exec ggtree_2.0.4.sif \
> >     rserver --www-port 8787 --www-address 0.0.0.0 --auth-none=0 --auth-pam-helper-path=pam-helper
> > ```
> > {: .bash}
> > 
> > ```
> > 
> > ```
> > {: .output}
> {: .solution}
> 
> Re-load the page in the web-browser.  
> Now you now what credentials to use for access.
> 
> What happens?
> 
> > ## Comments
> > 
> > You will get to the next screen, however an error message is popping up: **Error occurred during transmission**.  If you click on **OK** you'll get a blank screen.
> > 
> > If you check the terminal where you launched the server from, there's no error message.  Not really helpful.
> > 
> > Kill the session again with `Ctrl-C`.
> {: .solution}
{: .challenge}


> ## Third attempt: a writable location
> 
> What is happening is a consequence of the nature of Singularity containers: they're read only.  
> As many other packages, RStudio needs to write initialisation and configuration files in the user's home at runtime, which it cannot do in a Singularity container.
> 
> To work around this, you can bind mount a host directory to act as the user's home in the container, using for instance the `-B` flag.
> 
> Before you proceed, there's another matter to discuss, this time related to the way the `rocker` containers are setup.  These containers have a special user predefined in the Dockerfile, named `rstudio` and with user ID `1000`.  
> Get your user ID in the host, using the command `id -u`.  In Nimbus VMs by default it's a `1000`.  
> Now, because of this possible clash between user ID in the `rocker` containers and the one in the host, what happens is that the path of the `HOME` directory in the RStudio server will have a different value depending on the host user ID.  It will be `home/rstudio` if the host ID is `1000`, and will instead coincide with the *host* `$HOME` otherwise (for instance, Zeus users at Pawsey fall in this case).  
> 
> So, in the case of this workshop setup, you will need the container home to be `/home/rstudio`.  
> There's a directory ready to be used as fake home for RStudio in the current directory, `home_rstudio`.  So you could go with `-B home_rstudio:/home/rstudio`.
> 
> As a final note, if you don't want to bother about the two possibilities, you can actually go for a double mount of the host directory.  Singularity will let you do it and work correctly: `-B home_rstudio:/home/rstudio -B home_rstudio:$HOME`.
> 
> Now go ahead with this bind mounting!
> 
> > ## Solution
> > 
> > ```
> > $ export SINGULARITYENV_PASSWORD="<Pick-Your-Password>"
> > $ export SINGULARITYENV_USER="$USER"
> > $ singularity exec ggtree_2.0.4.sif \
> >     -B home_rstudio:/home/rstudio -B home_rstudio:$HOME \
> >     rserver --www-port 8787 --www-address 0.0.0.0 --auth-none=0 --auth-pam-helper-path=pam-helper
> > ```
> > {: .bash}
> > 
> > You don't need to redefine the shell variables, they're here to document them.
> {: .solution}
> 
> And now for another go with the web server on the browser.
> 
> > ## Comments
> > 
> > This time it seems to have worked.
> > 
> > You are getting the RStudio interface... you have just spawned a working RStudio server!
> > 
> > In the bottom right box, you can see there's the analysis file `script.r`.  
> > You can execute it by typing the following in the console on the left:
> > 
> > ```
> > > source('script.r')
> > ```
> > {: .R}
> > 
> > ...and after a few seconds, the phylogenetic tree is visualised on the right!
> > 
> > When you're done, click on the *Power Off` button at the top right of the RStudio interface.  You can either save the workspace data or not.  They will be saved in the fake home in the host, and remain available for future sessions if you need them.  
> > Then go back to the terminal and hit `Ctrl-C`.
> {: .solution}
{: .challenge}


<!--

> ## BONUS (HOME READING): One last addition, the temporary directory
> 
> **SKIP THIS** if you're running short of time.
> 
> Let's have a look at the content of the `/tmp` directory in the host VM.  
> This is a directory used by several processes to keep temporary files:
> 
> ```
> $ ls -l /tmp
> ```
> {: .bash}
> 
> ```
> [..]
> drwxrwxrwt 2 ubuntu ubuntu 4096 Jun  9 07:55 rstudio-rsession
> 
> [..]
> drwxr-xr-x 2 ubuntu ubuntu 4096 Jun  8 01:56 rstudio-server
> ```
> {: .output}
> 
> There are a couple of entries that were indeed created by the RStudio server.  This is not a problem in single user machines.  
> However, if you want to run this type of setup on a shared machine (any machine being used by multiple people), this can be an issue.  As you can see those directories do not have your username in their name, which means that when distinct users run RStudio servers, these will try to use the very same locations, possibly causing errors.
> 
> There are multiple ways to sort this out.  
> Today we're going to get this done by using the `singularity exec` flag `-C`, or `--containall`.  This option isolates the container from the host, including the use of a volatile `/tmp` directory instead of the host one, to better clean up the session upon exit.  In this way no entry will be written in the host `/tmp`.  
> As a by product, the shell environment is also isolated, which is why earlier on we defined `USER` and `PASSWORD` prefixing them with `SINGULARITYENV_`. 
> 
> Delete those temporary entries in `/tmp`:
> 
> ```
> $ rm -r /tmp/rstudio-*
> ```
> {: .bash}
> 
> Now launch the RServer one last time, adding the `-C` flag.
> 
> > ## Solution
> > 
> > ```
> > $ export SINGULARITYENV_PASSWORD="<Pick-Your-Password>"
> > $ export SINGULARITYENV_USER="$USER"
> > $ singularity exec ggtree_2.0.4.sif \
> >     -C \
> >     -B home_rstudio:/home/rstudio -B home_rstudio:$HOME \
> >     rserver --www-port 8787 --www-address 0.0.0.0 --auth-none=0 --auth-pam-helper-path=pam-helper
> > ```
> > {: .bash}
> {: .solution}
> 
> Login, type something in the console (*e.g. `source('script.r)`), then power off the session, and in the shell hit `Ctrl-C`.
> 
> Finally, check the content of `/tmp` with `ls -l /tmp`: this time there are no `rstudio` entries.
{: .challenge}

-->


### ROOM - Launch a Jupyter Notebook session

Another quite powerful and general-purpose graphical platform is Jupyter.  It provides a friendly interface to execute workflows in several language, the most common being Python and R.  Notably, these days a lot of Python visualisation libraries are being developed specifically for Jupyter, so that the only way to use them is on this platform.  

Here you need to visualise a network of gene correlations by using the package `ipycytoscape`.  There's a notebook ready for use at `home_jupyter/TipGeneExample.ipynb`.  This example notebook comes from the [Github page](https://github.com/QuantStack/ipycytoscape/blob/master/examples/Tip%20gene%20example.ipynb) of the package.  
Also, we've already made the image available on Docker Hub: `marcodelapierre/ipycytoscape:0.2.2`.  It is based on the image `jupyter/datascience-notebook:latest` by [Jupyter Stacks](https://jupyter-docker-stacks.readthedocs.io/en/latest/).

Similar to the RStudio example, you'll go through steps to work out the execution syntax for a containerised Jupyter Notebook.

First, download the image:

```
$ singularty pull docker://marcodelapierre/ipycytoscape:0.2.2
```
{: .bash}


> ## First: plain run
> 
> The standard command to start a Jupyter Notebook is:
> 
> ```
> jupyter notebook --no-browser --port=8888 --ip 0.0.0.0 --notebook-dir=$HOME
> ```
> {: .bash}
> 
> Here, `--no-browser` disables the creation of a window in the machine where the server is launched (in fact, you'll be connecting remotely using the web browser).  
> The flag `--notebook-dir` is handy to set the working directory.
> 
> Now, try and execute that command from the container.
> 
> > ## Solution
> > 
> > ```
> > $ singularity exec \
> >     ipycytoscape_0.2.2.sif \
> >     jupyter notebook --no-browser --port=8888 --ip 0.0.0.0 --notebook-dir=$HOME
> > ```
> > {: .bash}
> {: .solution}
> 
> What happens?
> 
> > ## Comments
> > 
> > You're getting lots of output and an error (well, at least Jupyter is verbose and tells you what's going on):
> > 
> > ```
> > [C 08:39:47.917 NotebookApp] Bad config encountered during initialization:
> > [C 08:39:47.917 NotebookApp] No such notebook dir: ''/home/ubuntu''
> > ```
> > {: .output}
> > 
> > This seems quite clear: you need to mount a writable home directory!
> {: .solution}
{: .challenge}


> ## Second: home directory
> 
> Note that in the Jupyter Stacks container images, you're known internally as the user `jovyan`.  However, the `HOME` directory with Singularity has the same path than the host.
> 
> Use the `-B` flag to mount `home_jupyter` as `$HOME`.  
> 
> > ## Solution
> > 
> > ```
> > $ singularity exec \
> >     -B home_jupyter:$HOME \
> >     ipycytoscape_0.2.2.sif \
> >     jupyter notebook --no-browser --port=8888 --ip 0.0.0.0 --notebook-dir=$HOME
> > ```
> > {: .bash}
> {: .solution}
> 
> What have you got?
> 
> > ## Comments
> > 
> > Well, yet another error message:
> > 
> > ```
> > OSError: [Errno 30] Read-only file system: '/run/user'
> > ```
> > {: .output}
> > 
> > The `/run` directory is a system directory used by some processes to store runtime files.  
> {: .solution}
{: .challenge}


> ## Third and last: container isolation
> 
> Files written in the `/run` directory are volatile, you don't really need them.  
> A simple turnaround is to use the container isolation variable `-C`, or `--containall`. This option isolates the container from the host, including the use of volatile `/run` and `/tmp` directories instead of the host ones, to better clean up the session upon exit.  In this way no entry will be written in the host `/run`.
> 
> Try and add the `-C` flag to the runtime:
> 
> > ## Solution
> > 
> > ```
> > $ singularity exec \
> >     -C \
> >     -B home_jupyter:$HOME \
> >     ipycytoscape_0.2.2.sif \
> >     jupyter notebook --no-browser --port=8888 --ip 0.0.0.0 --notebook-dir=$HOME
> > ```
> > {: .bash}
> {: .solution}
> 
> > ## Comments
> > 
> > It has worked!  
> > 
> > The terminal is hanging after having produced some relevant output:
> > 
> > ```
> > [..]
> > 
> > [I 08:47:16.450 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
> > [C 08:47:16.454 NotebookApp] 
> >     
> >     To access the notebook, open this file in a browser:
> >         file:///home/ubuntu/.local/share/jupyter/runtime/nbserver-16-open.html
> >     Or copy and paste one of these URLs:
> >         http://nimbus1:8888/?token=bde09af92ccaa4af48de43693ac3371b3fe9637f3a32c0b4
> >      or http://127.0.0.1:8888/?token=bde09af92ccaa4af48de43693ac3371b3fe9637f3a32c0b4
> > ```
> > {: .output}
> {: .solution}
{: .challenge}


Notice the long string after the keyword `token=` in the last line.  Select it and copy it in your clipboard.

Now, open your browser, and go to `http://<YOUR NIMBUS VM'S IP>:8888`.  
You're asked for a token.  Paste from your clipboard.

Click on the notebook `TipGeneExample.ipynb`.  
Then Click on the *Cell* menu, and then on the item *Run all*.

Scroll up a bit... here is your graph for the gene network!

When you're done, click on *Logout* on the top right of the page.  
Then go back to the terminal, hit `Ctrl-C`, enter `y`, and hit `Enter` to shutdown the server.


> ## ROOM - Feeder questions
> 
> * Look back at what you went through in the breakout room.  What was the most interesting learning outcome?  Which was the hardest bit to digest?
> * Do you use RStudio, Jupyter or other graphical web platforms?
> * Do you see advantages in terms of reproducibility, portability and collaboration, that a containerised approach can bring in using this type of platforms?
> * If you use RStudio or Jupyter, think at how many different packages and package dependency you normally need.  Do you reckon that a containerised approach may help in better managing large Python/R software stacks?
{: .discussion}


### GUIDED - Launch a containerised X11 application

Now we're going to see something new compared to the webinars.  





### Graphical applications using the X Window system

A good number of Linux scientific packages provide a graphical interface by means of desktop windows.  The *X Window System*, or *X11*, is one of the most common of this kind.  It allows an application to open graphical windows not only locally, but also remotely using a mechanism called *X11 forwarding.  In addition, dedicated clients exist for macOS and Windows that enable remote forwarding on this platforms.

In this episode we're going to learn how to run a X11 enabled application from inside a container.  As an example, we're using Gnuplot, a simple yet powerful plotting utility.


### Requirements for the host system (local or remote)

Regardless whether we're need to run our X11 application locally or remotely, our local machine needs an X11 system.  Linux boxes should all be good to go.  On macOS, you'll need to install [XQuartz](https://www.xquartz.org).  On Windows, there are a few alternatives; [Cygwin/X](https://x.cygwin.com) seems quite common.  
If we're running our application by remotely connecting to a Linux machine, that machine will need X11 setup, too.  Moreover, we'll need to enable X11 forwarding when establishing the SSH connection to that machine.  This is done via either

```
$ ssh -X
```
{: .bash}

or 

```
$ ssh -Y
```
{: .bash}

depending on whether we're connecting from Linux or macOS; in Windows, the graphical SSH client will have its own dedicated setting for X11 forwarding, that needs to be enabled.


### Extra package in the container image

There is one small requirement for the image that packages the X11 enabled application.  It's a utility called `xauth`, used to authenticate the X client in the container towards the X server of the local machine, so that the latter can go ahead in opening windows and receiving commands from the former.  
In practice, the Dockerfile (or def file) for the image needs to contain an `apt` command to install `xauth`.  *E.g.* in a Dockerfile:

```
[..]

RUN apt-get install -y xauth

[..]
```
{: .source}

To be precise, this additional package is not required when running X11 containerised applications with Singularity.  However, it would be required when running it with Docker, so it's good practice to embed it in order to enforce compatibility across different container engines.


### Extra flag when running the container with Singularity

One last thing to know to run our example is that we need to bind mount an *Xauthority* file in the container.  This file contains a secret string, or *magic cookie*, that is used by the X client to authenticate with the X server.  

Usually this file is located in the user's home directory, so the mounting will look like:

```
-B ~/.Xauthority
```
{: .bash}

Less often, a dedicated location is used for this file which is contained in the `$XAUTHORITY` variable.  In this case, this will do the job: `-B $XAUTHORITY`.


### Let's run a Gnuplot script and *see* what happens!

First remember that, in order for this example to work as intended, your local machine needs to have a X server installed (see above).  
Also, if you are logging in to a remote machine, you need to enable X11 forwarding using `ssh -X` (from Linux), or `ssh -Y` (from macOS), or from Windows whatever SSH client setting is required.  

Let's `cd` into the appropriate directory:

```
$ cd $TUTO/demos/gnuplot
```
{: .bash}

The sample script we're going to use comes from the Gnuplot Collection of Demo Scripts (see bottom of [this page](http://gnuplot.sourceforge.net/demo/hidden2.html)).  
We're going to use the image `docker://marcodelapierre/gnuplot:5.2.2_4`.  Let's start `gnuplot` from this image.  Don't forget the additional bind mounting:

```
$ singularity exec -B ~/.Xauthority docker://marcodelapierre/gnuplot:5.2.2_4 gnuplot
```
{: .bash}

```
gnuplot>
```
{: .output}

Now, from the gnuplot shell, execute the following:

```
gnuplot> load('tori.gp')
```
{: .source}

Whooa, we got doughnuts!

When you're done, close the X11 window by hitting `q`, and then close the gnuplot shell using `exit`.


